## BIG DATA AND CLOUD PLATFORMS

#### Building data pipelines (Storage)

---

## Data pipeline

Data pipeline
  > "A succession of operations to transform and consume raw data"

![https://xkcd.com/2054/](https://imgs.xkcd.com/comics/data_pipeline_2x.png)

<p class="footnote">
Quemy, Alexandre. "Data Pipeline Selection and Optimization." DOLAP. 2019.
</p>

---

## Data pipeline - AWS

![image](https://user-images.githubusercontent.com/18005592/210415573-a4dcb67e-e4ce-4fe2-9ca7-213a3e0ef5d4.png)

Three main categories
  * Ingest
  * Transform and store
  * Serve and consume

<p class="footnote">
<a href="https://console.aws.amazon.com/console" target="_blank">https://console.aws.amazon.com/console</a>
</p>

--

Three main categories
* Ingest
  * Gateway, DataSync (batch)
  * Kinesis, SNS, SQS (stream)
* Transform and store
  * S3 and Glacier (storage)
  * Glue (ETL)
* Serve and consume
  * EMR (Hadoop-like cluster)
  * Athena (serverless query service to analyze data in Amazon S3)
  * (Many) Machine learning services

---

## Data pipeline - Google cloud

![image](https://user-images.githubusercontent.com/18005592/210415914-7f830bfd-360c-42e2-997b-1c97cb622458.png)

Three main categories
  * Ingest
  * Analyze
  * Serve

-- 

Three main categories
* Ingest
  * Transfer service (batch)
  * Pub/Sub (stream)
* Analyze
  * Dataproc (batch)
  * Dataflow (stream)
  * Cloud storage (storage)
  * Machine learning services
* Serve
  * BigQuery (query service)

---

## A tentative organization

![image](https://user-images.githubusercontent.com/18005592/210415995-998fcd7a-8f4b-4fd7-abf4-391b9df8351d.png)

---

## A tentative organization

We have services
- To transform data
- To support the transformation

The (DIKW) pyramid abstracts many techniques and algorithms
- Standardization
- Integration
- Orchestration
- Accessibility through APIs

![image](https://user-images.githubusercontent.com/18005592/210416070-3d113437-383b-4dce-89fc-aedd721fa98c.png)

---

## Not a sharp taxonomy

![image](https://user-images.githubusercontent.com/18005592/210416410-2fe72a81-a927-44be-b9ff-17c296dda988.png)

Ingestion vs Analytics
  * Data streams are used for ingestion
  * ... and (event) processing

---

## Not a sharp taxonomy

![image](https://user-images.githubusercontent.com/18005592/210416439-b7b9ebfa-3faa-4775-afba-2bc58e9032f5.png)

Storage vs Serving
  * Databases are storage
  * ... with processing capability
  * ... and with serving capability

---

## Storage

![image](https://user-images.githubusercontent.com/18005592/210416684-1516f777-2cfe-47ea-af2e-cf5cb4ea965b.png)

--- 

## Storage

**Goal**: persisting data

Which storage do we choose?
- **Storage model** (or data model) ~= variety
  - How data are organized/accessed in a storage system
  - Structured vs unstructured
  - Data access model (key-value, column, etc.)
- **Access frequency**
- **Analyses** to be performed


---

## Storage models

![image](https://user-images.githubusercontent.com/18005592/210417048-a51191cf-2d9d-4b3d-bf92-f13b7fab1bd2.png)

<p class="footnote">
Mansouri, Yaser, Adel Nadjaran Toosi, and Rajkumar Buyya. "Data storage management in cloud environments: Taxonomy, survey, and future directions." ACM Computing Surveys (CSUR) 50.6 (2017): 1-51.
</p>

---

## Storage models (AWS)

Data structure: structured

Data abstraction: database

Data access model: relational

**Relational**
  * Store data with predefined schemas and relationships between them
  * Support ACID transactions
  * Maintain referential integrity

<p class="footnote">
<a href="https://aws.amazon.com/products/databases/" target="_blank">https://aws.amazon.com/products/databases/</a>
</p>

---

## Storage models (AWS)

![image](https://user-images.githubusercontent.com/18005592/210417519-b37819c3-fc2e-4aa4-81a2-c3bc6296a3ea.png)

---

## Storage models (AWS)

Data structure: semi/unstructured

Data abstraction: database

Data access model: *

  * **Key/value**: store and retrieve large volumes of data
  * **Document**: store semi-structured data as JSON-like documents
  * **Columnar**: use tables but unlike a relational database, columns can vary from row to row in the same table
  * **Graph**: navigate and query relationships between highly connected datasets
  * **... and more**

<p class="footnote">
<a href="https://aws.amazon.com/products/databases/" target="_blank">https://aws.amazon.com/products/databases/</a>
</p>

---

## Storage models (AWS)

![image](https://user-images.githubusercontent.com/18005592/210417569-43c9a5d3-8284-4a77-acb5-79d0b0bad646.png)


---

## Storage models (Google Cloud)

![image alt <](https://user-images.githubusercontent.com/18005592/210417914-8db53347-c54b-428d-b105-959fc1b376d8.png)

![image alt >](https://user-images.githubusercontent.com/18005592/210417733-12e4937b-5c09-4992-8e38-8d07586c336b.png)


<p class="footnote">
<a href="https://cloud.google.com/products/databases" target="_blank">https://cloud.google.com/products/databases</a>
</p>

---

## Storage models (AWS)

Data structure: unstructured

Data abstraction: file (or database)

Data access model: key-value

**File system** (EFS), **object storage** (S3) (or **DB K/V**; e.g., DynamoDB)
  * Handle unstructured data
  * ... organized as files (or blob)
  * ... accessed using a key-value

Differ in the supported features
  * E.g., maximum item size (DynamoDB: 400KB, S3: 5TB)
  * E.g., indexes, querying mechanisms, latency, etc.

---

## AWS S3

Simple Storage Service (S3)
- Serverless storage, save data as **objects** within **buckets**
- An **object** is composed of a file and any metadata that describes that file (e.g., object key)
- **Buckets** are logical containers for objects
- You can have one or more buckets in your account 
    - Control access for each bucket individually
    - Choose the geographical region where Amazon S3 will store the bucket

Benefits
- Unified data architecture
  - Build a multi-tenant environment, where many users can bring their own data 
  - Improve both cost and data governance over traditional solutions
- Decoupling of storage from compute and data processing 
  - You can cost-effectively store all data types in their native formats
  - Then, launch transformations as you need

---

## Object storage classes (AWS S3) 

- **Standard**: general purpose
- **Infrequent (rapid) access**
- **One Zone-IA**: lower-cost option for infrequently accessed data without high avail./resilience
* **Glacier**: low-cost storage for archiving, retrieval options that range from minutes to hours
* **Deep Glacier**: long-term retention for data accessed once or twice in a year
* **Intelligent-Tiering**: move objects between access tiers when access patterns change

<img src="https://user-images.githubusercontent.com/18005592/210418799-dd04e375-d0ad-4443-8e62-d668ad0c86ac.png"/>

<p class="footnote">
<a href="https://aws.amazon.com/s3/storage-classes/" target="_blank">https://aws.amazon.com/s3/storage-classes/</a>
</p>

---

# Lifecycle configuration

A set of rules that define actions that Amazon S3 applies to a group of objects
* **Transition** objects to another storage class
  * E.g., archive objects to the Glacier one year after creation
* **Expiration**: when objects expire. Amazon S3 deletes expired objects on your behalf

![image](https://user-images.githubusercontent.com/18005592/210419851-6f122ec2-022d-4af9-9464-2403e75689be.png)

<p class="footnote">
<a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html" target="_blank">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a>
</p>

---

## Storage: access frequency (Google Cloud)

![image alt <](https://user-images.githubusercontent.com/18005592/210420117-16ebe52d-8a82-4b85-b818-1c1219e219ef.png)

![image alt >](https://user-images.githubusercontent.com/18005592/210420091-3e9406b8-6e88-4726-bd11-3db3a34b011c.png)

<p class="footnote">
<a href="https://cloud.google.com/blog/products/storage-data-transfer/archive-storage-class-for-coldest-data-now-available" target="_blank">https://cloud.google.com/blog/products/storage-data-transfer/archive-storage-class-for-coldest-data-now-available</a>
</p>

---

## Organizing the data lake

Having a consistent principles on how to organize your data is important
  * To build standardized pipelines with the same design with regard to where read/write data
  * Standardization makes it easier to manage your pipelines at scale
  * Helps data users search for data in the storage and understand exactly to find what they need
  * Decoupling storage from processing

![image](https://user-images.githubusercontent.com/18005592/210421807-356e93d0-bc65-4f2a-92c1-97b0b93de41a.png)

---

Landing area (LA)
  * Save  <span style="color:#FF5050">raw data</span> from ingestion
  * Transient, data is not stored for long term

Staging area (SA)
  * Raw data goes through a set of common transformations: ensuring  <span style="color:#FF5050">basic quality</span> and making sure it  <span style="color:#FF5050">conforms to existing schemas</span>  for this data source and then data is saved into SA

Archive area (A)
  * After saving into SA, raw data from LA should be  <span style="color:#FF5050">copied into the archive</span> to reprocess any given batch of data by simply copying it from AA into LA
  * Useful for debugging and testing

--

![image](https://user-images.githubusercontent.com/18005592/210421970-34cba700-e9a3-4c05-a9c1-3a2ce805fd2b.png)

---

Production area (PA)
  * Apply the business logic to data from SA

Pass-through job
  * Copy data from SA to PA and then into DWH without applying any business logic
  * Optional, but having a data set in the data warehouse and PA that is an exact replica can be helpful when debugging any issues with the business logic

Cloud data warehouse (DWH)

Failed area (FA)
  * You need to be able to deal with all kinds of errors and failures
  * There might be bugs in the pipeline code, cloud resources may fail

--

![image](https://user-images.githubusercontent.com/18005592/210422002-7879aea4-bc4b-4cd4-9521-372bd2909903.png)

---

## Organizing the data lake

| Area | Permissions | Tier |
|:-: |:-: |:-: |
| Landing | Ingestion applications can write<br />Scheduled pipelines can readData consumers can't access | Hot |
| Staging | Scheduled pipelines can read/write<br />Selected data consumers can read | Hot |
| Production | Scheduled pipelines can read/writeSelected data consumers can read | Hot |
| Archive | Scheduled pipelines can writeDedicated data reprocessing pipelines can read  | Cold or archive |
| Failed | Scheduled pipelines can writeDedicated data reprocessing pipelines can readData consumers don't have access | Hot |

---

## Organizing the data lake

Use folders to organize data inside areas into a logical structure
  * <span style="color:#FF5050">Namespace</span>
    * Logically group multiple pipelines together
  * <span style="color:#00B050">Pipeline name</span>
    * Each data pipeline should have a name that reflects its purpose. For example
      * A pipeline that takes data from the LA, applies common processing steps, and saves data into SA
      * You will also have one for archiving data into AA
  * <span style="color:#0070C0">Data source name</span>
    * Ingestion layer will assign a name to each data source you bring into the platform
  * <span style="color:#7030A0">BatchId</span>
    * Unique identifier for any batch of data that is saved into LA
    * E.g., since only ingestion can write to LA, it is its responsibility to generate this identifier
    * A common choice for this type of an identifier is a Universally Unique Identifier (UUID)
Different areas will have slightly different folder structures
  * /landing/<span style="color:#FF5050">ETL</span>/<span style="color:#00B050">sales\_oracle\_ingest</span>/<span style="color:#0070C0">customers</span>/<span style="color:#7030A0">01DFTFX89YDFAXREPJTR94</span>